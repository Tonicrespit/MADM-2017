{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Not so) Big Data\n",
    "---\n",
    "En el siguiente enlace encontrarás un fichero de datos comprimido. [here](https://drive.google.com/open?id=1Kr8k8tmN2ziskPwLW_A8lQ_M2-5vHKsa)\n",
    "\n",
    "Descargalo y descomprimelo en tu local. Una vez hecho esto, realiza los siguientes ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instalación de Spark para Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1. Carga una sesión de spark local. Comprueba que el UI de Spark está activo en [localhost:4040](localhost:4040)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Con SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# crear sesión\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Python Spark SQL basic example\") \\\n",
    "        .config(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.0.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f48495505c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos la sesión de Spark se ha creado correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Crea un dataframe de Spark y carga el fichero. **\n",
    "\n",
    "Nota: esta separado por tabulador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos los datos, teniendo en cuenta que no tienen cabecera y que estan separados por tabulaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv('data/adds/adds.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3. Muestra la primera fila. ¿Cuántas variables numéricas hay?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos la primera fila del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, _c1=1, _c2=1, _c3=5, _c4=0, _c5=1382, _c6=4, _c7=15, _c8=2, _c9=181, _c10=1, _c11=2, _c12=None, _c13=2, _c14='68fd1e64', _c15='80e26c9b', _c16='fb936136', _c17='7b4723c4', _c18='25c83c98', _c19='7e0ccccf', _c20='de7995b8', _c21='1f89b562', _c22='a73ee510', _c23='a8cd5504', _c24='b2cb9c98', _c25='37c9c164', _c26='2824a5f6', _c27='1adce6ef', _c28='8ba8b39a', _c29='891b62e7', _c30='e5ba7672', _c31='f54016b9', _c32='21ddcdc9', _c33='b1252a9d', _c34='07b5194c', _c35=None, _c36='3a171ecb', _c37='c5c50484', _c38='e8b83407', _c39='9727dd16')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos la primera fila del data frame\n",
    "#Opción 1:\n",
    "df_spark.first()\n",
    "#Opción 2:\n",
    "df_spark.head(1)\n",
    "#Opción 3:\n",
    "df_spark.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos la estructura del conjunto de datos y el tipo de cada una de las variables. Como vemos, coincide con la descripción proporcionada, donde las 14 primeras variables son de tipo numérico, incluyendo la variable respuesta catalogada como **_c0** de tipo binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      " |-- _c29: string (nullable = true)\n",
      " |-- _c30: string (nullable = true)\n",
      " |-- _c31: string (nullable = true)\n",
      " |-- _c32: string (nullable = true)\n",
      " |-- _c33: string (nullable = true)\n",
      " |-- _c34: string (nullable = true)\n",
      " |-- _c35: string (nullable = true)\n",
      " |-- _c36: string (nullable = true)\n",
      " |-- _c37: string (nullable = true)\n",
      " |-- _c38: string (nullable = true)\n",
      " |-- _c39: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Inspeccionamos el Dataframe\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 4. Crea un dataframe de spark sólo con las variables numéricas `dfs_num` **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, seleccionamos las variables de tipo numérico y las guardamos en la estructura pertinente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " '_c1',\n",
       " '_c2',\n",
       " '_c3',\n",
       " '_c4',\n",
       " '_c5',\n",
       " '_c6',\n",
       " '_c7',\n",
       " '_c8',\n",
       " '_c9',\n",
       " '_c10',\n",
       " '_c11',\n",
       " '_c12',\n",
       " '_c13']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnList = [item[0] for item in df_spark.dtypes if item[1].startswith('int')]\n",
    "columnList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_num = df_spark[columnList]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, comprobamos que el cojunto obtenido es el requerido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: integer (nullable = true)\n",
      " |-- _c2: integer (nullable = true)\n",
      " |-- _c3: integer (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs_num.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5 ¿Cuántos valores nulos existen en cada columna? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+---+---+---+----+----+----+----+\n",
      "|_c0|_c1|_c2|_c3|_c4|_c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+----+----+----+\n",
      "|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|   0|   0|   0|   0|\n",
      "+---+---+---+---+---+---+---+---+---+---+----+----+----+----+\n",
      "\n",
      "+---+--------+---+-------+-------+-------+--------+-------+-----+-------+--------+-------+--------+-------+\n",
      "|_c0|     _c1|_c2|    _c3|    _c4|    _c5|     _c6|    _c7|  _c8|    _c9|    _c10|   _c11|    _c12|   _c13|\n",
      "+---+--------+---+-------+-------+-------+--------+-------+-----+-------+--------+-------+--------+-------+\n",
      "|  0|20793556|  0|9839447|9937369|1183117|10252328|1982866|22773|1982866|20793556|1982866|35071652|9937369|\n",
      "+---+--------+---+-------+-------+-------+--------+-------+-----+-------+--------+-------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, isnull\n",
    "\n",
    "# Miramos primero valores NAN\n",
    "dfs_num.select([count(when(isnan(c), c)).alias(c) for c in dfs_num.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miramos después valores isNull\n",
    "dfs_num.select([count(when(isnull(c), c)).alias(c) for c in dfs_num.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6. ¿Cuál es la media de cada una de las columnas? **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(summary='mean', _c0='0.2562233837297609', _c1='3.5024133170754044', _c2='105.84841979766546', _c3='26.913041020611274', _c4='7.322680248873305', _c5='18538.991664871523', _c6='116.06185085211598', _c7='16.333130032135028', _c8='12.517042137556713', _c9='106.1098234380509', _c10='0.6175294977722137', _c11='2.7328343170173044', _c12='0.9910356287721244', _c13='8.217461161174054')\n"
     ]
    }
   ],
   "source": [
    "df_describe = dfs_num.describe().collect()\n",
    "\n",
    "# Mostramos los valores de la media para cada columna\n",
    "print(df_describe[1])\n",
    "\n",
    "# Cogemos la fila que contiene los valores de la media\n",
    "# Convertimos los valores a entero\n",
    "mean_columns = list(map(float, list(df_describe[1][1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como alternativa, podemos realizar la misma operación de forma más directa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_columns = dfs_num.groupBy().mean().collect()\n",
    "print(mean_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 7. Sustituye los valores de cada columna por la media **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "for i in range(len(mean_columns)):\n",
    "    dfs_num = dfs_num.withColumn(dfs_num.columns[i], lit(mean_columns[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0.2562233837297609, _c1=3.5024133170754044, _c2=105.84841979766546, _c3=26.913041020611274, _c4=7.322680248873305, _c5=18538.991664871523, _c6=116.06185085211598, _c7=16.333130032135028, _c8=12.517042137556713, _c9=106.1098234380509, _c10=0.6175294977722137, _c11=2.7328343170173044, _c12=0.9910356287721244, _c13=8.217461161174054),\n",
       " Row(_c0=0.2562233837297609, _c1=3.5024133170754044, _c2=105.84841979766546, _c3=26.913041020611274, _c4=7.322680248873305, _c5=18538.991664871523, _c6=116.06185085211598, _c7=16.333130032135028, _c8=12.517042137556713, _c9=106.1098234380509, _c10=0.6175294977722137, _c11=2.7328343170173044, _c12=0.9910356287721244, _c13=8.217461161174054),\n",
       " Row(_c0=0.2562233837297609, _c1=3.5024133170754044, _c2=105.84841979766546, _c3=26.913041020611274, _c4=7.322680248873305, _c5=18538.991664871523, _c6=116.06185085211598, _c7=16.333130032135028, _c8=12.517042137556713, _c9=106.1098234380509, _c10=0.6175294977722137, _c11=2.7328343170173044, _c12=0.9910356287721244, _c13=8.217461161174054),\n",
       " Row(_c0=0.2562233837297609, _c1=3.5024133170754044, _c2=105.84841979766546, _c3=26.913041020611274, _c4=7.322680248873305, _c5=18538.991664871523, _c6=116.06185085211598, _c7=16.333130032135028, _c8=12.517042137556713, _c9=106.1098234380509, _c10=0.6175294977722137, _c11=2.7328343170173044, _c12=0.9910356287721244, _c13=8.217461161174054),\n",
       " Row(_c0=0.2562233837297609, _c1=3.5024133170754044, _c2=105.84841979766546, _c3=26.913041020611274, _c4=7.322680248873305, _c5=18538.991664871523, _c6=116.06185085211598, _c7=16.333130032135028, _c8=12.517042137556713, _c9=106.1098234380509, _c10=0.6175294977722137, _c11=2.7328343170173044, _c12=0.9910356287721244, _c13=8.217461161174054)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_num.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:green'>Bonus<span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark permite entrenar modelos de ML sobre grandes volúmenes de datos en paralelo. Échale un vistazo a la documentación [here](https://spark.apache.org/docs/2.2.0/ml-classification-regression.html#binomial-logistic-regression) e intenta crear un modelo de clasificación usando sólo las variables numéricas anteriores.\n",
    "\n",
    "Utiliza los últimos 10M de filas como set de testeo.\n",
    "\n",
    "La variable objetivo esta en la primera columna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Crea un modelo de clasificación en Spark usando solo las variables numéricas. ¿Qué AUC ROC obtienes?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 1.0, 5.0, 0.0, 1382.0, 4.0, 15.0, 2.0, 181.0, 1.0, 2.0, nan]), label=0),\n",
       " Row(features=DenseVector([2.0, 0.0, 44.0, 1.0, 102.0, 8.0, 2.0, 2.0, 4.0, 1.0, 1.0, nan]), label=0),\n",
       " Row(features=DenseVector([2.0, 0.0, 1.0, 14.0, 767.0, 89.0, 4.0, 2.0, 245.0, 1.0, 3.0, 3.0]), label=0)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Load training data\n",
    "training=dfs_num.rdd.map(lambda x:(Vectors.dense(x[1:-1]), x[0])).toDF([\"features\", \"label\"])\n",
    "training.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[1.0,1.0,5.0,0.0,...|    0|\n",
      "|[2.0,0.0,44.0,1.0...|    0|\n",
      "|[2.0,0.0,1.0,14.0...|    0|\n",
      "|[NaN,893.0,NaN,Na...|    0|\n",
      "|[3.0,-1.0,NaN,0.0...|    0|\n",
      "|[NaN,-1.0,NaN,NaN...|    0|\n",
      "|[NaN,1.0,2.0,NaN,...|    0|\n",
      "|[1.0,4.0,2.0,0.0,...|    1|\n",
      "|[NaN,44.0,4.0,8.0...|    0|\n",
      "|[NaN,35.0,NaN,1.0...|    0|\n",
      "|[NaN,2.0,632.0,0....|    0|\n",
      "|[0.0,6.0,6.0,6.0,...|    0|\n",
      "|[0.0,-1.0,NaN,NaN...|    1|\n",
      "|[NaN,2.0,11.0,5.0...|    1|\n",
      "|[0.0,51.0,84.0,4....|    0|\n",
      "|[NaN,2.0,1.0,18.0...|    0|\n",
      "|[1.0,987.0,NaN,2....|    1|\n",
      "|[0.0,1.0,NaN,0.0,...|    0|\n",
      "|[0.0,24.0,4.0,2.0...|    0|\n",
      "|[7.0,102.0,NaN,3....|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
